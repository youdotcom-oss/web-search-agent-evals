{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ACP Agent Evaluation Results\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/plaited/acp-evals/blob/main/notebooks/summary.ipynb)\n",
        "\n",
        "**Public benchmark comparing agent web search capabilities.**\n",
        "\n",
        "This notebook visualizes results from the latest evaluation run. GitHub renders this automatically - scroll down to see charts. Click the Colab badge above to explore the data interactively.\n",
        "\n",
        "**Raw data:** All results are committed in `/data/results/runs/` for reproducibility.\n",
        "\n",
        "**Analysis includes:**\n",
        "- Overall quality rankings (weighted strategy)\n",
        "- Head-to-head win/loss matrix\n",
        "- Statistical significance testing (bootstrap)\n",
        "- Raw metrics (pass rates, latency, errors)\n",
        "- Historical trends across runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# Read latest.json to find current run\n",
        "with open('../data/results/latest.json') as f:\n",
        "    latest = json.load(f)\n",
        "\n",
        "print(f\"Analyzing run: {latest['date']}\")\n",
        "print(f\"Prompts: {latest['promptCount']}\")\n",
        "print(f\"Commit: {latest['commit']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data\n",
        "\n",
        "Load raw trajectory results and comparison analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read agents and search providers from manifest\n",
        "with open('../data/results/MANIFEST.jsonl') as f:\n",
        "    lines = f.readlines()\n",
        "    latest_entry = json.loads(lines[-1])  # Get most recent run\n",
        "    agents = latest_entry['agents']\n",
        "    search_providers = latest_entry['searchProviders']\n",
        "\n",
        "# Load raw trajectory results\n",
        "results = {}\n",
        "for agent in agents:\n",
        "    results[agent] = {}\n",
        "    for provider in search_providers:\n",
        "        path = f\"../data/results/{latest['path']}/{agent}/{provider}.jsonl\"\n",
        "        try:\n",
        "            with open(path) as f:\n",
        "                results[agent][provider] = [json.loads(line) for line in f]\n",
        "        except FileNotFoundError:\n",
        "            print(f\"\u26a0\ufe0f  Missing: {path}\")\n",
        "            results[agent][provider] = []\n",
        "\n",
        "# Flatten to DataFrame for raw metrics\n",
        "rows = []\n",
        "for agent, providers_dict in results.items():\n",
        "    for provider, result_list in providers_dict.items():\n",
        "        for r in result_list:\n",
        "            rows.append({\n",
        "                'agent': agent,\n",
        "                'search_provider': provider,\n",
        "                'id': r['id'],\n",
        "                'score': r.get('score', 0),\n",
        "                'pass': r.get('score', 0) >= 0.7,  # Match grader threshold (70%)\n",
        "                'latency_ms': r.get('timing', {}).get('total', 0),\n",
        "                'tool_errors': r.get('toolErrors', False),\n",
        "            })\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "# Load comparison analysis (weighted strategy)\n",
        "comparison_path = f\"../data/comparisons/{latest['path']}/all-weighted.json\"\n",
        "try:\n",
        "    with open(comparison_path) as f:\n",
        "        comparison = json.load(f)\n",
        "    \n",
        "    # Transform quality dict into rankings\n",
        "    quality_items = [(run, metrics['avgScore']) \n",
        "                     for run, metrics in comparison['quality'].items()]\n",
        "    rankings_df = pd.DataFrame(quality_items, columns=['run', 'score'])\n",
        "    rankings_df = rankings_df.sort_values('score', ascending=False).reset_index(drop=True)\n",
        "    rankings_df['rank'] = rankings_df.index + 1\n",
        "    \n",
        "    # Transform head-to-head pairwise into enhanced format\n",
        "    h2h_enhanced = []\n",
        "    for pair in comparison['headToHead']['pairwise']:\n",
        "        winner = pair['runA'] if pair['aWins'] > pair['bWins'] else pair['runB']\n",
        "        loser = pair['runB'] if pair['aWins'] > pair['bWins'] else pair['runA']\n",
        "        total_decisive = pair['aWins'] + pair['bWins']\n",
        "        confidence = max(pair['aWins'], pair['bWins']) / total_decisive if total_decisive > 0 else 0.5\n",
        "        h2h_enhanced.append({\n",
        "            'winner': winner,\n",
        "            'loser': loser,\n",
        "            'confidence': confidence,\n",
        "            'aWins': pair['aWins'],\n",
        "            'bWins': pair['bWins'],\n",
        "            'ties': pair['ties']\n",
        "        })\n",
        "    \n",
        "    quality = comparison['quality']\n",
        "    print(f\"\u2713 Loaded comparison analysis with {len(rankings_df)} runs\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"\u26a0\ufe0f  No comparison data found at {comparison_path}\")\n",
        "    print(\"   Run: bun scripts/compare.ts --mode full\")\n",
        "    quality = None\n",
        "    rankings_df = None\n",
        "    h2h_enhanced = None\n",
        "\n",
        "print(f\"\u2713 Loaded {len(df)} trajectory results\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overall Rankings\n",
        "\n",
        "Quality rankings from weighted comparison strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if rankings_df is not None:\n",
        "    # Show quality rankings with scores\n",
        "    rankings_df['score_pct'] = rankings_df['score'] * 100\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    bars = ax.barh(rankings_df['run'], rankings_df['score_pct'])\n",
        "\n",
        "    # Color bars by score (green > 50%, red < 50%)\n",
        "    for i, bar in enumerate(bars):\n",
        "        if rankings_df.iloc[i]['score_pct'] >= 50:\n",
        "            bar.set_color('green')\n",
        "        else:\n",
        "            bar.set_color('red')\n",
        "\n",
        "    ax.axvline(50, color='black', linestyle='--', linewidth=1, label='50% baseline')\n",
        "    ax.set_xlabel('Quality Score (%)')\n",
        "    ax.set_title('Overall Quality Rankings (Weighted Strategy)')\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nTop performers:\")\n",
        "    for _, row in rankings_df.head(3).iterrows():\n",
        "        print(f\"  {row['rank']}. {row['run']}: {row['score_pct']:.1f}%\")\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f  Skipping rankings chart - no comparison data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Head-to-Head Comparison Matrix\n",
        "\n",
        "Win confidence matrix from pairwise comparisons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if h2h_enhanced:\n",
        "    # Build win matrix from pairwise comparisons\n",
        "    runs = list(set([p['winner'] for p in h2h_enhanced] + [p['loser'] for p in h2h_enhanced]))\n",
        "    n = len(runs)\n",
        "    matrix = np.zeros((n, n))\n",
        "\n",
        "    run_to_idx = {run: i for i, run in enumerate(runs)}\n",
        "    for pair in h2h_enhanced:\n",
        "        winner_idx = run_to_idx[pair['winner']]\n",
        "        loser_idx = run_to_idx[pair['loser']]\n",
        "        matrix[winner_idx, loser_idx] = pair['confidence']\n",
        "\n",
        "    # Heatmap\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(matrix, xticklabels=runs, yticklabels=runs, annot=True, fmt='.2f',\n",
        "                cmap='RdYlGn', center=0.5, vmin=0, vmax=1, cbar_kws={'label': 'Win Confidence'})\n",
        "    plt.title('Head-to-Head Win Confidence Matrix\\n(row wins against column with confidence score)')\n",
        "    plt.xlabel('Loser')\n",
        "    plt.ylabel('Winner')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\nSignificant wins (confidence > 0.95):\")\n",
        "    for pair in h2h_enhanced:\n",
        "        if pair['confidence'] > 0.95:\n",
        "            print(f\"  {pair['winner']} > {pair['loser']} ({pair['confidence']:.3f})\")\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f  Skipping head-to-head matrix - no comparison data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pass Rates by Agent and Search Provider\n",
        "\n",
        "Success rate (pass=True from grader) by agent/provider combination."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pass_rates = df.groupby(['agent', 'search_provider'])['pass'].mean() * 100\n",
        "\n",
        "pass_rates.unstack().plot(kind='bar', figsize=(10, 6))\n",
        "plt.title('Pass Rate (%) by Agent and Search Provider')\n",
        "plt.ylabel('Pass Rate (%)')\n",
        "plt.xlabel('Agent')\n",
        "plt.legend(title='Search Provider')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Latency Distribution\n",
        "\n",
        "Response time histograms for each agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "for idx, agent in enumerate(agents):\n",
        "    ax = axes[idx // 2, idx % 2]\n",
        "    for provider in search_providers:\n",
        "        data = df[(df['agent'] == agent) & (df['search_provider'] == provider)]['latency_ms'] / 1000\n",
        "        if len(data) > 0:\n",
        "            ax.hist(data, bins=30, alpha=0.6, label=provider)\n",
        "    ax.set_title(f'{agent.title()} - Latency Distribution')\n",
        "    ax.set_xlabel('Latency (seconds)')\n",
        "    ax.set_ylabel('Frequency')\n",
        "    ax.legend(title='Search Provider')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tool Error Rates\n",
        "\n",
        "Percentage of queries with tool errors by agent/provider."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "error_rates = df.groupby(['agent', 'search_provider'])['tool_errors'].mean() * 100\n",
        "\n",
        "error_rates.unstack().plot(kind='bar', figsize=(10, 6))\n",
        "plt.title('Tool Error Rate (%) by Agent and Search Provider')\n",
        "plt.ylabel('Error Rate (%)')\n",
        "plt.xlabel('Agent')\n",
        "plt.legend(title='Search Provider')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Historical Trends\n",
        "\n",
        "Performance over time (if multiple runs exist)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load all runs from MANIFEST\n",
        "with open('../data/results/MANIFEST.jsonl') as f:\n",
        "    all_runs = [json.loads(line) for line in f]\n",
        "\n",
        "if len(all_runs) > 1:\n",
        "    # Track quality scores over time for each agent-provider combo\n",
        "    historical_data = []\n",
        "\n",
        "    for run_entry in all_runs:\n",
        "        run_date = run_entry['date']\n",
        "        run_path = f\"runs/{run_date}\"\n",
        "\n",
        "        # Try to load comparison for this run\n",
        "        try:\n",
        "            comp_path = f\"../data/comparisons/{run_path}/all-weighted.json\"\n",
        "            with open(comp_path) as f:\n",
        "                comp = json.load(f)\n",
        "\n",
        "            # Build rankings from quality dict\n",
        "            for run_name, metrics in comp['quality'].items():\n",
        "                historical_data.append({\n",
        "                    'date': run_date,\n",
        "                    'run': run_name,\n",
        "                    'score': metrics['avgScore'] * 100,\n",
        "                })\n",
        "        except FileNotFoundError:\n",
        "            print(f\"\u26a0\ufe0f  No comparison data for {run_date}\")\n",
        "\n",
        "    if historical_data:\n",
        "        hist_df = pd.DataFrame(historical_data)\n",
        "\n",
        "        # Plot trends for top performers\n",
        "        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "        for run_name in hist_df['run'].unique():\n",
        "            run_data = hist_df[hist_df['run'] == run_name]\n",
        "            ax.plot(run_data['date'], run_data['score'], marker='o', label=run_name)\n",
        "\n",
        "        ax.set_xlabel('Run Date')\n",
        "        ax.set_ylabel('Quality Score (%)')\n",
        "        ax.set_title('Quality Score Trends Over Time')\n",
        "        ax.axhline(50, color='black', linestyle='--', linewidth=1)\n",
        "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"Only one run in history. Historical trends will appear after future runs.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}