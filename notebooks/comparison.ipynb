{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Comparison Analysis\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/youdotcom-oss/web-search-agent-evals/blob/main/notebooks/comparison.ipynb)\n",
    "\n",
    "Visualize comparison results from weighted and statistical analysis of web search agent evaluations.\n",
    "\n",
    "## What This Analyzes\n",
    "\n",
    "This notebook visualizes pre-computed comparison metrics from:\n",
    "- **Weighted Strategy**: Balances quality (70%), latency (20%), reliability (10%)\n",
    "- **Statistical Strategy**: Bootstrap sampling with significance testing (p<0.05)\n",
    "\n",
    "**Agents Compared**: Claude Code, Gemini, Droid, Codex  \n",
    "**Search Tools**: builtin, You.com MCP  \n",
    "**Configurations**: 8 total (4 agents √ó 2 tools)\n",
    "\n",
    "## Quick Navigation\n",
    "\n",
    "1. [Setup & Data Loading](#setup)\n",
    "2. [Overall Rankings](#rankings)\n",
    "3. [Quality vs Performance](#quality-perf)\n",
    "4. [Head-to-Head Matrix](#head-to-head)\n",
    "5. [Statistical Significance](#significance)\n",
    "6. [Search Provider Comparison](#provider)\n",
    "7. [Pass Rate Analysis](#pass-rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Colab Setup (auto-detects environment)\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect if running in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"üîß Running in Google Colab - cloning repository...\")\n",
    "    \n",
    "    # Clone repository if not already present\n",
    "    repo_dir = Path('/content/web-search-agent-evals')\n",
    "    if not repo_dir.exists():\n",
    "        !git clone https://github.com/youdotcom-oss/web-search-agent-evals.git /content/web-search-agent-evals\n",
    "        print(\"‚úì Repository cloned\")\n",
    "    else:\n",
    "        print(\"‚úì Repository already exists\")\n",
    "        # Pull latest changes\n",
    "        %cd /content/web-search-agent-evals\n",
    "        !git pull origin main\n",
    "    \n",
    "    # Change to repo directory\n",
    "    %cd /content/web-search-agent-evals\n",
    "    print(f\"‚úì Working directory: {Path.cwd()}\")\n",
    "else:\n",
    "    print(\"‚úì Running locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Dependencies & Configuration\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure plotting\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Find project root\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if PROJECT_ROOT.name == 'notebooks':\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "print(f\"üìÅ Project root: {PROJECT_ROOT}\")\n",
    "print(f\"üìä Data directory: {DATA_DIR}\")\n",
    "\n",
    "# Verify data directory exists\n",
    "if not DATA_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Data directory not found: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Configuration - Choose Dataset Mode\n",
    "\n",
    "# =====================================\n",
    "# USER CONFIGURATION\n",
    "# =====================================\n",
    "MODE = 'test'  # Options: 'test' or 'full'\n",
    "RUN_DATE = None  # For full mode: '2026-01-24' or None for latest\n",
    "# =====================================\n",
    "\n",
    "print(f\"üìä MODE: {MODE.upper()}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if MODE == 'test':\n",
    "    comp_dir = DATA_DIR / 'comparisons' / 'test-runs'\n",
    "    print(\"Loading test mode comparisons (5 prompts, rapid iteration)\")\n",
    "elif MODE == 'full':\n",
    "    if RUN_DATE:\n",
    "        comp_dir = DATA_DIR / 'comparisons' / 'runs' / RUN_DATE\n",
    "        print(f\"Loading full run: {RUN_DATE}\")\n",
    "    else:\n",
    "        # Find most recent run\n",
    "        runs_dir = DATA_DIR / 'comparisons' / 'runs'\n",
    "        if runs_dir.exists():\n",
    "            latest_date = sorted(d.name for d in runs_dir.iterdir() if d.is_dir())[-1]\n",
    "            comp_dir = runs_dir / latest_date\n",
    "            print(f\"Loading latest full run: {latest_date}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No full runs found in {runs_dir}\")\n",
    "else:\n",
    "    raise ValueError(f\"Invalid MODE: {MODE}. Must be 'test' or 'full'\")\n",
    "\n",
    "print(f\"Comparison directory: {comp_dir}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "## Load Comparison Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load Weighted Comparison\n",
    "weighted_file = comp_dir / 'all-weighted.json'\n",
    "\n",
    "if not weighted_file.exists():\n",
    "    raise FileNotFoundError(f\"Weighted comparison not found: {weighted_file}\\n\"\n",
    "                            f\"Run: bun run compare --mode {MODE}\")\n",
    "\n",
    "with open(weighted_file) as f:\n",
    "    weighted = json.load(f)\n",
    "\n",
    "print(\"‚úì Loaded weighted comparison\")\n",
    "print(f\"  Configurations: {len(weighted['quality'])}\")\n",
    "print(f\"  Strategy: {weighted['meta']['strategy']}\")\n",
    "print(f\"  Timestamp: {weighted['meta']['timestamp']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Load Statistical Comparison\n",
    "statistical_file = comp_dir / 'all-statistical.json'\n",
    "\n",
    "if statistical_file.exists():\n",
    "    with open(statistical_file) as f:\n",
    "        statistical = json.load(f)\n",
    "    print(\"‚úì Loaded statistical comparison\")\n",
    "    print(f\"  Bootstrap iterations: {statistical['meta'].get('bootstrapIterations', 'N/A')}\")\n",
    "    HAS_STATISTICAL = True\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No statistical comparison found\")\n",
    "    print(f\"   Run: bun run compare --mode {MODE} --strategy statistical\")\n",
    "    statistical = None\n",
    "    HAS_STATISTICAL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Prepare DataFrames\n",
    "\n",
    "# Extract rankings from weighted comparison\n",
    "rankings = []\n",
    "for config, metrics in weighted['quality'].items():\n",
    "    rankings.append({\n",
    "        'config': config,\n",
    "        'avgScore': metrics['avgScore'],\n",
    "        'passRate': metrics['passRate'],\n",
    "        'passCount': metrics['passCount'],\n",
    "        'failCount': metrics['failCount'],\n",
    "        'agent': config.split('-')[0] if '-' in config else config,\n",
    "        'provider': config.split('-')[1] if '-' in config and len(config.split('-')) > 1 else 'unknown'\n",
    "    })\n",
    "\n",
    "rankings_df = pd.DataFrame(rankings).sort_values('avgScore', ascending=False)\n",
    "rankings_df['rank'] = range(1, len(rankings_df) + 1)\n",
    "\n",
    "# Extract performance metrics\n",
    "perf_data = []\n",
    "for config, metrics in weighted['performance'].items():\n",
    "    perf_data.append({\n",
    "        'config': config,\n",
    "        'p50_latency': metrics['latency']['p50'],\n",
    "        'p90_latency': metrics['latency']['p90'],\n",
    "        'p99_latency': metrics['latency']['p99']\n",
    "    })\n",
    "\n",
    "perf_df = pd.DataFrame(perf_data)\n",
    "\n",
    "# Merge quality and performance\n",
    "full_df = rankings_df.merge(perf_df, on='config')\n",
    "\n",
    "print(\"‚úì Prepared analysis dataframes\")\n",
    "print(f\"  {len(rankings_df)} configurations analyzed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rankings'></a>\n",
    "## Overall Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Rankings Bar Chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Color code: top 3 green, rest blue\n",
    "colors = ['#2ecc71' if i < 3 else '#3498db' for i in range(len(rankings_df))]\n",
    "bars = ax.barh(rankings_df['config'], rankings_df['avgScore'] * 100, color=colors)\n",
    "\n",
    "ax.set_xlabel('Average Score (%)', fontsize=12)\n",
    "ax.set_ylabel('Configuration', fontsize=12)\n",
    "ax.set_title(f'Agent Rankings by Quality Score ({MODE.upper()} mode)', fontsize=14, fontweight='bold')\n",
    "ax.axvline(x=65, color='red', linestyle='--', alpha=0.5, label='Pass Threshold (65%)')\n",
    "ax.legend()\n",
    "\n",
    "# Add score labels\n",
    "for bar, score in zip(bars, rankings_df['avgScore']):\n",
    "    ax.text(score * 100 + 1, bar.get_y() + bar.get_height()/2,\n",
    "            f\"{score*100:.1f}%\", va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print top 3\n",
    "print(\"\\nüèÜ TOP 3 CONFIGURATIONS\")\n",
    "print(\"=\"*70)\n",
    "for _, row in rankings_df.head(3).iterrows():\n",
    "    print(f\"#{int(row['rank'])} {row['config']}\")\n",
    "    print(f\"   Score: {row['avgScore']:.1%} | Pass Rate: {row['passRate']:.1%} ({row['passCount']}/{row['passCount']+row['failCount']})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='quality-perf'></a>\n",
    "## Quality vs Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Quality vs Latency Scatter Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Create scatter plot with provider-based colors\n",
    "providers = full_df['provider'].unique()\n",
    "colors_map = {'builtin': '#3498db', 'you': '#e74c3c'}\n",
    "\n",
    "for provider in providers:\n",
    "    df_subset = full_df[full_df['provider'] == provider]\n",
    "    ax.scatter(df_subset['p50_latency'], df_subset['avgScore'] * 100,\n",
    "               s=150, alpha=0.6, label=provider,\n",
    "               color=colors_map.get(provider, '#95a5a6'))\n",
    "\n",
    "# Add labels for each point\n",
    "for _, row in full_df.iterrows():\n",
    "    ax.annotate(row['agent'], \n",
    "                (row['p50_latency'], row['avgScore'] * 100),\n",
    "                textcoords=\"offset points\", xytext=(0,10), ha='center',\n",
    "                fontsize=9)\n",
    "\n",
    "ax.set_xlabel('Median Latency (ms)', fontsize=12)\n",
    "ax.set_ylabel('Quality Score (%)', fontsize=12)\n",
    "ax.set_title('Quality vs Performance Tradeoff', fontsize=14, fontweight='bold')\n",
    "ax.axhline(y=65, color='red', linestyle='--', alpha=0.3, label='Pass Threshold')\n",
    "ax.legend(title='Search Provider', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Ideal: Top-left quadrant (high quality, low latency)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='head-to-head'></a>\n",
    "## Head-to-Head Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Head-to-Head Win Rate Matrix\n",
    "pairwise = weighted['headToHead']['pairwise']\n",
    "configs = rankings_df['config'].tolist()\n",
    "\n",
    "# Build win rate matrix\n",
    "n = len(configs)\n",
    "win_matrix = np.zeros((n, n))\n",
    "\n",
    "for i, config_a in enumerate(configs):\n",
    "    for j, config_b in enumerate(configs):\n",
    "        if i == j:\n",
    "            win_matrix[i, j] = 0.5  # Diagonal\n",
    "        else:\n",
    "            key = f\"{config_a} vs {config_b}\"\n",
    "            if key in pairwise:\n",
    "                record = pairwise[key]\n",
    "                total = record['wins'] + record['losses'] + record['ties']\n",
    "                win_rate = (record['wins'] + 0.5 * record['ties']) / total if total > 0 else 0\n",
    "                win_matrix[i, j] = win_rate\n",
    "\n",
    "# Plot heatmap\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(win_matrix, annot=True, fmt='.2f', cmap='RdYlGn', center=0.5,\n",
    "            xticklabels=configs, yticklabels=configs, ax=ax,\n",
    "            cbar_kws={'label': 'Win Rate'}, vmin=0, vmax=1)\n",
    "\n",
    "ax.set_title('Head-to-Head Win Rate Matrix\\n(Row vs Column)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Opponent', fontsize=12)\n",
    "ax.set_ylabel('Agent', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° How to read: Each cell shows win rate of row agent vs column agent\")\n",
    "print(\"   Green (>0.5) = Row agent wins more often\")\n",
    "print(\"   Red (<0.5) = Column agent wins more often\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='significance'></a>\n",
    "## Statistical Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Confidence Intervals (if statistical data available)\n",
    "if HAS_STATISTICAL:\n",
    "    ci_data = []\n",
    "    for config, metrics in statistical['quality'].items():\n",
    "        ci = metrics['confidenceInterval']\n",
    "        ci_data.append({\n",
    "            'config': config,\n",
    "            'mean': metrics['avgScore'],\n",
    "            'ci_lower': ci['lower'],\n",
    "            'ci_upper': ci['upper'],\n",
    "            'ci_width': ci['upper'] - ci['lower']\n",
    "        })\n",
    "    \n",
    "    ci_df = pd.DataFrame(ci_data).sort_values('mean', ascending=False)\n",
    "    \n",
    "    # Plot confidence intervals\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    y_pos = range(len(ci_df))\n",
    "    ax.errorbar(ci_df['mean'] * 100, y_pos, \n",
    "                xerr=[(ci_df['mean'] - ci_df['ci_lower']) * 100, \n",
    "                      (ci_df['ci_upper'] - ci_df['mean']) * 100],\n",
    "                fmt='o', capsize=5, capthick=2, markersize=8)\n",
    "    \n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(ci_df['config'])\n",
    "    ax.set_xlabel('Score (%) with 95% Confidence Interval', fontsize=12)\n",
    "    ax.set_title('Statistical Confidence Intervals\\n(Bootstrap with 1000 iterations)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.axvline(x=65, color='red', linestyle='--', alpha=0.3, label='Pass Threshold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüí° Overlapping intervals = difference may not be statistically significant\")\n",
    "    print(\"   Narrow intervals = more reliable estimate\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Statistical analysis not available\")\n",
    "    print(f\"   Run: bun run compare --mode {MODE} --strategy statistical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='provider'></a>\n",
    "## Search Provider Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Provider Comparison by Agent\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Group by agent and provider\n",
    "agent_provider = full_df.groupby(['agent', 'provider'])['avgScore'].mean().unstack()\n",
    "\n",
    "# Score comparison\n",
    "agent_provider_pct = agent_provider * 100\n",
    "agent_provider_pct.plot(kind='bar', ax=ax1, color=['#3498db', '#e74c3c'])\n",
    "ax1.set_title('Quality Score by Search Provider', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Score (%)', fontsize=12)\n",
    "ax1.set_xlabel('Agent', fontsize=12)\n",
    "ax1.axhline(y=65, color='red', linestyle='--', alpha=0.3, label='Pass Threshold')\n",
    "ax1.legend(title='Provider')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Latency comparison\n",
    "latency_provider = full_df.groupby(['agent', 'provider'])['p50_latency'].mean().unstack()\n",
    "latency_provider.plot(kind='bar', ax=ax2, color=['#3498db', '#e74c3c'])\n",
    "ax2.set_title('Median Latency by Search Provider', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('Latency (ms)', fontsize=12)\n",
    "ax2.set_xlabel('Agent', fontsize=12)\n",
    "ax2.legend(title='Provider')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print provider winner per agent\n",
    "print(\"\\nüèÖ PROVIDER WINNER PER AGENT (Quality)\")\n",
    "print(\"=\"*70)\n",
    "for agent in agent_provider.index:\n",
    "    winner = agent_provider.loc[agent].idxmax()\n",
    "    score_diff = (agent_provider.loc[agent, winner] - agent_provider.loc[agent].min()) * 100\n",
    "    print(f\"{agent}: {winner} (+{score_diff:.1f}% better)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pass-rates'></a>\n",
    "## Pass Rate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Pass Rates\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Color code by pass rate\n",
    "colors = ['#2ecc71' if rate >= 0.5 else '#f39c12' if rate >= 0.3 else '#e74c3c'\n",
    "          for rate in rankings_df['passRate']]\n",
    "\n",
    "bars = ax.barh(rankings_df['config'], rankings_df['passRate'] * 100, color=colors)\n",
    "\n",
    "ax.set_xlabel('Pass Rate (%)', fontsize=12)\n",
    "ax.set_ylabel('Configuration', fontsize=12)\n",
    "ax.set_title(f'Pass Rates (Score ‚â• 65%)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add percentage labels\n",
    "for bar, rate, count in zip(bars, rankings_df['passRate'], rankings_df['passCount']):\n",
    "    ax.text(rate * 100 + 1, bar.get_y() + bar.get_height()/2,\n",
    "            f\"{rate*100:.1f}% ({int(count)})\", va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nüìä PASS RATE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Best: {rankings_df.iloc[0]['config']} ({rankings_df.iloc[0]['passRate']:.1%})\")\n",
    "print(f\"Worst: {rankings_df.iloc[-1]['config']} ({rankings_df.iloc[-1]['passRate']:.1%})\")\n",
    "print(f\"Average: {rankings_df['passRate'].mean():.1%}\")\n",
    "print(f\"Median: {rankings_df['passRate'].median():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook visualized comparison results. For deeper analysis:\n",
    "- **Raw trajectories**: Load individual JSONL files from `data/results/`\n",
    "- **Trials analysis**: See `trials.ipynb` for pass@k reliability metrics\n",
    "- **Custom comparisons**: Use `bun run compare` with different flags"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
