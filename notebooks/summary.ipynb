{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Agentic Web Search Playoffs: Evaluation Results\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/youdotcom-oss/agentic-web-search-playoffs/blob/YOUR_BRANCH/notebooks/summary.ipynb)\n-->\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/youdotcom-oss/agentic-web-search-playoffs/blob/main/notebooks/summary.ipynb)\n\n**Comparing 4 agents √ó 2 search tools = 8 configurations across 1,254 web search tasks**\n\n## Quick Navigation\n\n**üìä Key Findings**\n1. [Executive Summary](#Executive-Summary) - Top performers and key observations\n2. [Overall Rankings](#Overall-Rankings) - Quality rankings across configurations\n\n**üî¨ Deep Analysis**\n3. [Score Distribution](#Score-Distribution-Analysis) - Understanding grading patterns\n4. [MCP Tool Usage](#MCP-Tool-Usage-Analysis) - MCP integration verification\n5. [Statistical Significance](#Statistical-Significance-Testing) - Bootstrap confidence\n\n**üìà Performance Deep-Dive**\n6. [Head-to-Head Matrix](#Head-to-Head-Comparison-Matrix) - Pairwise win/loss\n7. [Pass Rates](#Pass-Rates-by-Configuration) - Success rates by agent/tool\n8. [Latency Analysis](#Latency-Distribution) - Response time distributions\n9. [Error Rates](#Tool-Error-Rates) - Tool failure analysis\n\n**üìÖ Historical Context**\n10. [Trends Over Time](#Historical-Trends) - Performance evolution\n\n---\n\n**Methodology:** Hybrid grading (deterministic 60% + LLM 40%), pass threshold ‚â•70%  \n**Data:** Raw trajectories and comparisons in `/data/results/runs/` for reproducibility"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Environment Setup\n\n**‚ö†Ô∏è Repository Visibility:** This repository is currently INTERNAL. To use this notebook in Google Colab:\n- **Option 1:** Make the repository PUBLIC at [repository settings](https://github.com/youdotcom-oss/agentic-web-search-playoffs/settings)\n- **Option 2:** Use GitHub authentication (see error message below if clone fails)\n\n**For local Jupyter users:** This cell will automatically find the project root."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nfrom pathlib import Path\n\n# Detect environment\nIN_COLAB = 'google.colab' in sys.modules\nREPO_URL = 'https://github.com/youdotcom-oss/agentic-web-search-playoffs.git'\nREPO_DIR = 'agentic-web-search-playoffs'\n\nif IN_COLAB:\n    print(\"üîµ Google Colab detected\")\n    \n    # Clone repo if not already cloned\n    if not Path(REPO_DIR).exists():\n        print(f\"üì• Cloning {REPO_URL}...\")\n        result = !git clone {REPO_URL} 2>&1\n        \n        # Check if clone succeeded\n        if Path(REPO_DIR).exists():\n            print(\"‚úì Repository cloned\")\n        else:\n            print(\"\\n‚ùå Failed to clone repository\")\n            print(\"\\n‚ö†Ô∏è  This repository is INTERNAL and requires authentication.\")\n            print(\"\\nOptions:\")\n            print(\"1. Make the repository PUBLIC at: https://github.com/youdotcom-oss/agentic-web-search-playoffs/settings\")\n            print(\"2. OR manually upload the data/ folder to Colab\")\n            print(\"3. OR use GitHub token authentication:\")\n            print(\"   !git clone https://YOUR_TOKEN@github.com/youdotcom-oss/agentic-web-search-playoffs.git\")\n            raise FileNotFoundError(f\"Could not clone {REPO_URL}\")\n    else:\n        print(f\"‚úì Repository already exists at {REPO_DIR}\")\n    \n    # Change to repo directory\n    os.chdir(REPO_DIR)\n    print(f\"üìÅ Working directory: {Path.cwd()}\")\n    \n    # Install dependencies\n    print(\"\\nüì¶ Installing Python dependencies...\")\n    !pip install -q pandas matplotlib seaborn numpy\n    print(\"‚úì Dependencies installed\")\nelse:\n    print(\"üíª Local Jupyter detected\")\n    \n    # Find project root (contains data/results/latest.json)\n    current_dir = Path.cwd()\n    project_root = None\n    \n    # If in notebooks/ directory, go up one level\n    if current_dir.name == 'notebooks':\n        project_root = current_dir.parent\n    else:\n        # Search for project root\n        for parent in [current_dir] + list(current_dir.parents):\n            if (parent / 'data' / 'results' / 'latest.json').exists():\n                project_root = parent\n                break\n    \n    if project_root:\n        os.chdir(project_root)\n        print(f\"‚úì Project root: {Path.cwd()}\")\n    else:\n        print(f\"‚ö†Ô∏è  Could not find project root with data/results/latest.json\")\n        print(f\"   Current directory: {Path.cwd()}\")\n\nprint(f\"\\n‚úì Ready to load data from: {Path.cwd() / 'data' / 'results' / 'latest.json'}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Data Loading\n\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Configure plotting style\nsns.set_style('whitegrid')\nplt.rcParams['figure.dpi'] = 100\n\n# Read latest run metadata\nwith open('data/results/latest.json') as f:\n    latest = json.load(f)\n\nprint(f\"üìä Loading data from run: {latest['date']}\")\nprint(f\"   Prompts: {latest['promptCount']:,}\")\nprint(f\"   Commit: {latest['commit']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Read agents and search providers from manifest\nwith open('data/results/MANIFEST.jsonl') as f:\n    lines = f.readlines()\n    latest_entry = json.loads(lines[-1])\n    agents = latest_entry['agents']\n    search_providers = latest_entry['searchProviders']\n\nprint(f\"Agents: {', '.join(agents)}\")\nprint(f\"Search providers: {', '.join(search_providers)}\")\n\n# Load raw trajectory results\nresults = {}\nfor agent in agents:\n    results[agent] = {}\n    for provider in search_providers:\n        path = f\"data/results/{latest['path']}/{agent}/{provider}.jsonl\"\n        try:\n            with open(path) as f:\n                results[agent][provider] = [json.loads(line) for line in f]\n        except FileNotFoundError:\n            print(f\"‚ö†Ô∏è  Missing: {path}\")\n            results[agent][provider] = []\n\n# Flatten to DataFrame\nrows = []\nfor agent, providers_dict in results.items():\n    for provider, result_list in providers_dict.items():\n        for r in result_list:\n            rows.append({\n                'agent': agent,\n                'search_provider': provider,\n                'id': r['id'],\n                'score': r.get('score', 0),\n                'pass': r.get('score', 0) >= 0.7,\n                'latency_ms': r.get('timing', {}).get('total', 0),\n                'tool_errors': r.get('toolErrors', False),\n            })\n\ndf = pd.DataFrame(rows)\nprint(f\"\\n‚úì Loaded {len(df):,} trajectory results\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary\n",
    "\n",
    "High-level findings and key takeaways from this evaluation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rankings_df is not None:\n",
    "    print(f\"üìä EVALUATION RUN: {latest['date']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Prompts Evaluated: {latest['promptCount']:,}\")\n",
    "    print(f\"Configurations: {len(rankings_df)} (4 agents √ó 2 search tools)\")\n",
    "    print(f\"Data Commit: {latest['commit']}\")\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"TOP 3 CONFIGURATIONS (by weighted average score)\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    for _, row in rankings_df.head(3).iterrows():\n",
    "        score_pct = row['score'] * 100\n",
    "        pass_pct = row['passRate'] * 100\n",
    "        print(f\"#{row['rank']} {row['run']}\")\n",
    "        print(f\"    Avg Score: {score_pct:.2f}% | Pass Rate: {pass_pct:.2f}%\\n\")\n",
    "    \n",
    "    print(f\"{'='*70}\")\n",
    "    print(\"KEY OBSERVATIONS\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Score distribution\n",
    "    score_min, score_max = rankings_df['score'].min() * 100, rankings_df['score'].max() * 100\n",
    "    score_spread = score_max - score_min\n",
    "    print(f\"üìä Score Distribution\")\n",
    "    print(f\"   Range: {score_min:.2f}% - {score_max:.2f}% (spread: {score_spread:.2f}pp)\")\n",
    "    if score_spread < 5:\n",
    "        print(f\"   ‚ö†Ô∏è  Narrow spread suggests score clustering - may need grader calibration\")\n",
    "    \n",
    "    # Pass rate variance\n",
    "    pass_min, pass_max = rankings_df['passRate'].min() * 100, rankings_df['passRate'].max() * 100\n",
    "    print(f\"\\n‚úì Pass Rates\")\n",
    "    print(f\"   Range: {pass_min:.2f}% - {pass_max:.2f}%\")\n",
    "    if pass_max > 10 * pass_min:\n",
    "        print(f\"   üìå Wide variance indicates distinct performance tiers\")\n",
    "    \n",
    "    # Tool reliability\n",
    "    error_by_config = df.groupby(['agent', 'search_provider'])['tool_errors'].mean() * 100\n",
    "    print(f\"\\nüîß Tool Reliability\")\n",
    "    print(f\"   Error Rate Range: {error_by_config.min():.1f}% - {error_by_config.max():.1f}%\")\n",
    "    if error_by_config.max() > 20:\n",
    "        worst = error_by_config.idxmax()\n",
    "        print(f\"   ‚ö†Ô∏è  {worst[0]}-{worst[1]}: {error_by_config.max():.1f}% error rate (reliability issue)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Comparison data not available. Run: bun scripts/compare.ts --mode full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Rankings\n",
    "\n",
    "Quality rankings by average score (deterministic + LLM hybrid grading)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rankings_df is not None:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    colors = ['#2ecc71' if i < 3 else '#3498db' for i in range(len(rankings_df))]\n",
    "    bars = ax.barh(rankings_df['run'], rankings_df['score'] * 100, color=colors)\n",
    "    \n",
    "    ax.set_xlabel('Average Score (%)', fontsize=12)\n",
    "    ax.set_ylabel('Configuration', fontsize=12)\n",
    "    ax.set_title('Agent + Search Provider Rankings (Higher is Better)', fontsize=14, fontweight='bold')\n",
    "    ax.axvline(x=70, color='red', linestyle='--', alpha=0.5, label='Pass Threshold (70%)')\n",
    "    ax.legend()\n",
    "    \n",
    "    for i, (bar, score) in enumerate(zip(bars, rankings_df['score'])):\n",
    "        ax.text(score * 100 + 0.5, bar.get_y() + bar.get_height()/2, \n",
    "                f\"{score*100:.2f}%\", va='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nFull Rankings Table:\")\n",
    "    display(rankings_df[['rank', 'run', 'score', 'passRate']].style.format({'score': '{:.4f}', 'passRate': '{:.4f}'}))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No comparison data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Distribution Analysis\n",
    "\n",
    "Understanding how scores are distributed helps identify:\n",
    "- **Bimodal patterns**: Agent either succeeds or fails completely\n",
    "- **Clustering**: Grader may need calibration if scores bunch up\n",
    "- **Outliers**: Unusually high/low performance on specific prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent-provider label\n",
    "df['config'] = df['agent'] + '-' + df['search_provider']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Violin plot: Score distribution by configuration\n",
    "sns.violinplot(data=df, y='config', x='score', ax=axes[0], inner='quartile')\n",
    "axes[0].axvline(x=0.7, color='red', linestyle='--', alpha=0.5, label='Pass Threshold')\n",
    "axes[0].set_xlabel('Score', fontsize=12)\n",
    "axes[0].set_ylabel('Configuration', fontsize=12)\n",
    "axes[0].set_title('Score Distribution by Configuration\\n(wider = more variance)', fontsize=13, fontweight='bold')\n",
    "axes[0].legend()\n",
    "\n",
    "# Box plot: Quartile view\n",
    "sns.boxplot(data=df, y='config', x='score', ax=axes[1])\n",
    "axes[1].axvline(x=0.7, color='red', linestyle='--', alpha=0.5, label='Pass Threshold')\n",
    "axes[1].set_xlabel('Score', fontsize=12)\n",
    "axes[1].set_ylabel('')\n",
    "axes[1].set_title('Quartile View\\n(box = IQR, whiskers = 1.5√óIQR)', fontsize=13, fontweight='bold')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\nüìä SCORE DISTRIBUTION INSIGHTS\")\n",
    "print(\"=\"*70)\n",
    "for config in df['config'].unique():\n",
    "    config_scores = df[df['config'] == config]['score']\n",
    "    q1, median, q3 = config_scores.quantile([0.25, 0.5, 0.75])\n",
    "    iqr = q3 - q1\n",
    "    skew = config_scores.skew()\n",
    "    \n",
    "    print(f\"\\n{config}:\")\n",
    "    print(f\"  Q1: {q1:.3f} | Median: {median:.3f} | Q3: {q3:.3f} | IQR: {iqr:.3f}\")\n",
    "    print(f\"  Skew: {skew:.2f}\", end='')\n",
    "    if abs(skew) < 0.5:\n",
    "        print(\" (symmetric)\")\n",
    "    elif skew > 0:\n",
    "        print(\" (right-skewed: more low scores)\")\n",
    "    else:\n",
    "        print(\" (left-skewed: more high scores)\")\n",
    "    \n",
    "    # Detect clustering\n",
    "    if iqr < 0.15:\n",
    "        print(f\"  ‚ö†Ô∏è Narrow IQR suggests score clustering - 50% of results within {iqr:.3f} range\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCP Tool Usage Analysis\n",
    "\n",
    "Verify that configurations using You.com MCP actually called the MCP tool (not builtin).  \n",
    "**Why this matters**: If MCP configs fall back to builtin search, results aren't valid comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check trajectory metadata for MCP usage\n",
    "mcp_usage = []\n",
    "for agent in agents:\n",
    "    for provider in search_providers:\n",
    "        if provider == 'builtin':\n",
    "            continue  # Skip builtin configs\n",
    "        \n",
    "        result_list = results.get(agent, {}).get(provider, [])\n",
    "        if not result_list:\n",
    "            continue\n",
    "        \n",
    "        mcp_calls = 0\n",
    "        builtin_calls = 0\n",
    "        no_search_calls = 0\n",
    "        \n",
    "        for r in result_list:\n",
    "            trajectory = r.get('trajectory', [])\n",
    "            has_mcp = False\n",
    "            has_builtin = False\n",
    "            \n",
    "            for step in trajectory:\n",
    "                if step.get('type') == 'tool_call':\n",
    "                    tool_name = step.get('name', '')\n",
    "                    # Check for MCP tool patterns\n",
    "                    if 'mcp' in tool_name.lower() or 'ydc' in tool_name.lower() or 'you.com' in tool_name.lower():\n",
    "                        has_mcp = True\n",
    "                    # Check for builtin patterns\n",
    "                    elif 'websearch' in tool_name.lower() or 'search' in tool_name.lower():\n",
    "                        has_builtin = True\n",
    "            \n",
    "            if has_mcp:\n",
    "                mcp_calls += 1\n",
    "            elif has_builtin:\n",
    "                builtin_calls += 1\n",
    "            else:\n",
    "                no_search_calls += 1\n",
    "        \n",
    "        total = len(result_list)\n",
    "        mcp_usage.append({\n",
    "            'config': f\"{agent}-{provider}\",\n",
    "            'expected_mcp': provider,\n",
    "            'total_prompts': total,\n",
    "            'mcp_calls': mcp_calls,\n",
    "            'builtin_calls': builtin_calls,\n",
    "            'no_search': no_search_calls,\n",
    "            'mcp_rate': mcp_calls / total if total > 0 else 0,\n",
    "        })\n",
    "\n",
    "if mcp_usage:\n",
    "    mcp_df = pd.DataFrame(mcp_usage)\n",
    "    \n",
    "    print(\"üîç MCP TOOL USAGE VERIFICATION\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"Expected: Configurations with 'you' provider should use You.com MCP tool\\n\")\n",
    "    \n",
    "    display(mcp_df[['config', 'total_prompts', 'mcp_calls', 'builtin_calls', 'no_search', 'mcp_rate']]\n",
    "            .style.format({'mcp_rate': '{:.1%}'}))\n",
    "    \n",
    "    # Detect issues\n",
    "    print(\"\\nüìã FINDINGS:\")\n",
    "    for _, row in mcp_df.iterrows():\n",
    "        if row['mcp_rate'] < 0.5:\n",
    "            print(f\"‚ö†Ô∏è {row['config']}: Only {row['mcp_rate']:.1%} used MCP (expected: you provider)\")\n",
    "            print(f\"   ‚Üí {row['builtin_calls']} fell back to builtin, {row['no_search']} had no search\")\n",
    "        elif row['mcp_rate'] > 0.9:\n",
    "            print(f\"‚úÖ {row['config']}: {row['mcp_rate']:.1%} MCP usage (good)\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è {row['config']}: {row['mcp_rate']:.1%} MCP usage (mixed - investigate)\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No MCP configurations found in this run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Significance Testing\n",
    "\n",
    "Bootstrap sampling (1000 iterations) to determine if performance differences are statistically significant (p < 0.05).  \n",
    "**Key**: Differences must be significant to claim one agent/tool is truly better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if stat_comparison is not None:\n",
    "    print(\"üìä STATISTICAL SIGNIFICANCE RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"Bootstrap Method: 1000 iterations | Significance Level: p < 0.05\\n\")\n",
    "    \n",
    "    # Extract significant pairs from statistical comparison\n",
    "    significant_pairs = []\n",
    "    \n",
    "    if 'headToHead' in stat_comparison and 'pairwise' in stat_comparison['headToHead']:\n",
    "        for pair in stat_comparison['headToHead']['pairwise']:\n",
    "            # Statistical comparison includes p-values and confidence intervals\n",
    "            if 'pValue' in pair and pair.get('pValue', 1.0) < 0.05:\n",
    "                significant_pairs.append({\n",
    "                    'runA': pair['runA'],\n",
    "                    'runB': pair['runB'],\n",
    "                    'pValue': pair['pValue'],\n",
    "                    'significant': True,\n",
    "                    'aWins': pair.get('aWins', 0),\n",
    "                    'bWins': pair.get('bWins', 0),\n",
    "                })\n",
    "    \n",
    "    if significant_pairs:\n",
    "        sig_df = pd.DataFrame(significant_pairs)\n",
    "        print(f\"Found {len(sig_df)} statistically significant pairwise differences:\\n\")\n",
    "        \n",
    "        for _, row in sig_df.iterrows():\n",
    "            winner = row['runA'] if row['aWins'] > row['bWins'] else row['runB']\n",
    "            loser = row['runB'] if row['aWins'] > row['bWins'] else row['runA']\n",
    "            print(f\"‚úÖ {winner} > {loser}\")\n",
    "            print(f\"   p-value: {row['pValue']:.4f} (wins: {max(row['aWins'], row['bWins'])} vs {min(row['aWins'], row['bWins'])})\\n\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No statistically significant differences found (p < 0.05)\")\n",
    "        print(\"   This suggests:\")\n",
    "        print(\"   - Performance differences may be due to chance\")\n",
    "        print(\"   - Need larger sample size or more diverse test set\")\n",
    "        print(\"   - Grader may need calibration to differentiate quality\\n\")\n",
    "    \n",
    "    # Show confidence intervals if available\n",
    "    if 'quality' in stat_comparison:\n",
    "        print(\"\\n95% CONFIDENCE INTERVALS (Bootstrap)\")\n",
    "        print(\"=\"*70)\n",
    "        ci_data = []\n",
    "        for run, metrics in stat_comparison['quality'].items():\n",
    "            if 'confidenceInterval' in metrics:\n",
    "                ci = metrics['confidenceInterval']\n",
    "                ci_data.append({\n",
    "                    'run': run,\n",
    "                    'avgScore': metrics['avgScore'],\n",
    "                    'ci_lower': ci['lower'],\n",
    "                    'ci_upper': ci['upper'],\n",
    "                    'ci_width': ci['upper'] - ci['lower'],\n",
    "                })\n",
    "        \n",
    "        if ci_data:\n",
    "            ci_df = pd.DataFrame(ci_data).sort_values('avgScore', ascending=False)\n",
    "            display(ci_df.style.format({\n",
    "                'avgScore': '{:.4f}',\n",
    "                'ci_lower': '{:.4f}',\n",
    "                'ci_upper': '{:.4f}',\n",
    "                'ci_width': '{:.4f}'\n",
    "            }))\n",
    "            \n",
    "            print(\"\\nüìä Narrower CI width = more consistent performance\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Statistical comparison not available.\")\n",
    "    print(\"   Run: bun scripts/compare.ts --mode full --strategy statistical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Head-to-Head Comparison Matrix\n",
    "\n",
    "Pairwise win/loss comparison across all configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if h2h_enhanced is not None:\n",
    "    # Build matrix\n",
    "    configs = sorted(df['config'].unique())\n",
    "    matrix = pd.DataFrame(0, index=configs, columns=configs)\n",
    "    \n",
    "    for item in h2h_enhanced:\n",
    "        winner, loser = item['winner'], item['loser']\n",
    "        if winner in matrix.index and loser in matrix.columns:\n",
    "            matrix.loc[winner, loser] = item['confidence']\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    sns.heatmap(matrix, annot=True, fmt='.2f', cmap='RdYlGn', center=0.5, \n",
    "                vmin=0, vmax=1, ax=ax, cbar_kws={'label': 'Win Confidence'})\n",
    "    ax.set_title('Head-to-Head Win Matrix (row > column)', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Loser', fontsize=12)\n",
    "    ax.set_ylabel('Winner', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nInterpretation: Each cell shows win confidence (0.5 = tied, 1.0 = always wins)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No head-to-head data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pass Rates by Configuration\n",
    "\n",
    "Percentage of prompts with score ‚â• 70% (pass threshold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_rates = df.groupby('config')['pass'].mean().sort_values(ascending=True) * 100\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = ['#e74c3c' if rate < 10 else '#f39c12' if rate < 20 else '#2ecc71' for rate in pass_rates]\n",
    "bars = ax.barh(pass_rates.index, pass_rates.values, color=colors)\n",
    "\n",
    "ax.set_xlabel('Pass Rate (%)', fontsize=12)\n",
    "ax.set_ylabel('Configuration', fontsize=12)\n",
    "ax.set_title('Pass Rates (Score ‚â• 70%)', fontsize=14, fontweight='bold')\n",
    "\n",
    "for bar, rate in zip(bars, pass_rates.values):\n",
    "    ax.text(rate + 0.5, bar.get_y() + bar.get_height()/2, \n",
    "            f\"{rate:.2f}%\", va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPass Rate Summary:\")\n",
    "print(pass_rates.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latency Distribution\n",
    "\n",
    "Response time distributions by configuration (median and p90)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latency_stats = df.groupby('config')['latency_ms'].agg(['median', lambda x: x.quantile(0.9)]).rename(columns={'<lambda_0>': 'p90'})\n",
    "latency_stats = latency_stats.sort_values('median')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = range(len(latency_stats))\n",
    "ax.barh(x, latency_stats['median'], label='Median', alpha=0.7)\n",
    "ax.barh(x, latency_stats['p90'], label='P90', alpha=0.5)\n",
    "\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(latency_stats.index)\n",
    "ax.set_xlabel('Latency (ms)', fontsize=12)\n",
    "ax.set_ylabel('Configuration', fontsize=12)\n",
    "ax.set_title('Latency Distribution (Lower is Better)', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nLatency Summary (ms):\")\n",
    "print(latency_stats.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Error Rates\n",
    "\n",
    "Percentage of prompts where tool calls failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rates = df.groupby('config')['tool_errors'].mean().sort_values(ascending=False) * 100\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = ['#e74c3c' if rate > 20 else '#f39c12' if rate > 10 else '#2ecc71' for rate in error_rates]\n",
    "bars = ax.barh(error_rates.index, error_rates.values, color=colors)\n",
    "\n",
    "ax.set_xlabel('Error Rate (%)', fontsize=12)\n",
    "ax.set_ylabel('Configuration', fontsize=12)\n",
    "ax.set_title('Tool Error Rates (Lower is Better)', fontsize=14, fontweight='bold')\n",
    "\n",
    "for bar, rate in zip(bars, error_rates.values):\n",
    "    ax.text(rate + 0.5, bar.get_y() + bar.get_height()/2, \n",
    "            f\"{rate:.2f}%\", va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nError Rate Summary:\")\n",
    "print(error_rates.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical Trends\n",
    "\n",
    "Performance evolution across evaluation runs (if multiple runs available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Read all runs from MANIFEST\nmanifest_path = Path('data/results/MANIFEST.jsonl')\nif manifest_path.exists():\n    with open(manifest_path) as f:\n        runs = [json.loads(line) for line in f]\n    \n    if len(runs) > 1:\n        print(f\"üìÖ Found {len(runs)} evaluation runs\\n\")\n        \n        # Load comparison data for each run\n        historical_data = []\n        for run_meta in runs:\n            if run_meta.get('dataset') != 'full':\n                continue  # Skip test runs\n            \n            comp_path = Path(f\"data/comparisons/{run_meta['path']}/all-weighted.json\")\n            if comp_path.exists():\n                with open(comp_path) as f:\n                    comp = json.load(f)\n                \n                for run_name, metrics in comp['quality'].items():\n                    historical_data.append({\n                        'date': run_meta['date'],\n                        'config': run_name,\n                        'avgScore': metrics['avgScore'],\n                        'passRate': metrics['passRate'],\n                    })\n        \n        if historical_data:\n            hist_df = pd.DataFrame(historical_data)\n            \n            fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n            \n            # Average score trends\n            for config in hist_df['config'].unique():\n                config_data = hist_df[hist_df['config'] == config].sort_values('date')\n                axes[0].plot(config_data['date'], config_data['avgScore'] * 100, marker='o', label=config)\n            \n            axes[0].set_xlabel('Date', fontsize=12)\n            axes[0].set_ylabel('Average Score (%)', fontsize=12)\n            axes[0].set_title('Score Trends Over Time', fontsize=13, fontweight='bold')\n            axes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n            axes[0].grid(True, alpha=0.3)\n            axes[0].tick_params(axis='x', rotation=45)\n            \n            # Pass rate trends\n            for config in hist_df['config'].unique():\n                config_data = hist_df[hist_df['config'] == config].sort_values('date')\n                axes[1].plot(config_data['date'], config_data['passRate'] * 100, marker='o', label=config)\n            \n            axes[1].set_xlabel('Date', fontsize=12)\n            axes[1].set_ylabel('Pass Rate (%)', fontsize=12)\n            axes[1].set_title('Pass Rate Trends Over Time', fontsize=13, fontweight='bold')\n            axes[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n            axes[1].grid(True, alpha=0.3)\n            axes[1].tick_params(axis='x', rotation=45)\n            \n            plt.tight_layout()\n            plt.show()\n        else:\n            print(\"‚ÑπÔ∏è No historical full run data available for comparison\")\n    else:\n        print(\"‚ÑπÔ∏è Only one evaluation run available - no trends to show yet\")\nelse:\n    print(\"‚ÑπÔ∏è No MANIFEST.jsonl found\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
