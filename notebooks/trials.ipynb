{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pass@k Trials Analysis\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/youdotcom-oss/web-search-agent-evals/blob/main/notebooks/trials.ipynb)\n",
    "\n",
    "Deep dive into agent reliability through pass@k metrics from multi-trial evaluations.\n",
    "\n",
    "## What This Analyzes\n",
    "\n",
    "This notebook analyzes **trials data** where each prompt is run multiple times (k trials) to measure:\n",
    "- **pass@k (Capability)**: Can the agent do this task at all?\n",
    "- **pass^k (Reliability)**: Does it always succeed?\n",
    "- **Flakiness**: How much variance across trials?\n",
    "\n",
    "**Typical Configuration**:\n",
    "- **Capability Mode**: k=10 trials (can it solve this?)\n",
    "- **Default Mode**: k=5 trials (balanced)\n",
    "- **Regression Mode**: k=3 trials (faster checks)\n",
    "\n",
    "## Quick Navigation\n",
    "\n",
    "1. [Setup & Data Loading](#setup)\n",
    "2. [Pass Rate Distribution](#pass-rates)\n",
    "3. [Capability vs Reliability](#capability-reliability)\n",
    "4. [Flakiness Analysis](#flakiness)\n",
    "5. [Prompt Difficulty](#difficulty)\n",
    "6. [Per-Prompt Heatmap](#heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Colab Setup (auto-detects environment)\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect if running in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"üîß Running in Google Colab - cloning repository...\")\n",
    "    \n",
    "    # Clone repository if not already present\n",
    "    repo_dir = Path('/content/web-search-agent-evals')\n",
    "    if not repo_dir.exists():\n",
    "        !git clone https://github.com/youdotcom-oss/web-search-agent-evals.git /content/web-search-agent-evals\n",
    "        print(\"‚úì Repository cloned\")\n",
    "    else:\n",
    "        print(\"‚úì Repository already exists\")\n",
    "        # Pull latest changes\n",
    "        %cd /content/web-search-agent-evals\n",
    "        !git pull origin main\n",
    "    \n",
    "    # Change to repo directory\n",
    "    %cd /content/web-search-agent-evals\n",
    "    print(f\"‚úì Working directory: {Path.cwd()}\")\n",
    "else:\n",
    "    print(\"‚úì Running locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Dependencies & Configuration\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure plotting\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Find project root\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if PROJECT_ROOT.name == 'notebooks':\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "print(f\"üìÅ Project root: {PROJECT_ROOT}\")\n",
    "print(f\"üìä Data directory: {DATA_DIR}\")\n",
    "\n",
    "# Verify data directory exists\n",
    "if not DATA_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Data directory not found: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Configuration - Choose Trials Dataset\n\n# =====================================\n# USER CONFIGURATION\n# =====================================\nAGENT = 'droid'          # Options: 'claude-code', 'gemini', 'droid', 'codex'\nPROVIDER = 'builtin'     # Options: 'builtin', 'you' (or other MCP server keys)\nTRIAL_TYPE = 'default'   # Options: 'default', 'capability', 'regression'\nRUN_DATE = None          # None for latest, or '2026-01-29' for specific date\n# =====================================\n\nprint(f\"üìä TRIALS ANALYSIS: {AGENT.upper()} - {PROVIDER.upper()}\")\nprint(\"=\"*70)\n\ntrials_dir = DATA_DIR / 'results' / 'trials'\n\n# Get latest date if not specified\nif RUN_DATE is None:\n    dirs = sorted([d.name for d in trials_dir.iterdir() if d.is_dir() and d.name[0].isdigit()])\n    if not dirs:\n        raise FileNotFoundError(\"No dated trials found\")\n    RUN_DATE = dirs[-1]\n    print(f\"Using latest trials run: {RUN_DATE}\")\nelse:\n    print(f\"Using specified run date: {RUN_DATE}\")\n\n# Show available dates\navailable_dates = sorted([d.name for d in trials_dir.iterdir() if d.is_dir() and d.name[0].isdigit()])\nprint(f\"\\nAvailable trial dates:\")\nfor date in available_dates[-5:]:  # Show last 5\n    print(f\"  - {date}\")\n\n# Build trials file path (same nested structure as runs)\nsuffix = '' if TRIAL_TYPE == 'default' else f'-{TRIAL_TYPE}'\ntrials_file = trials_dir / RUN_DATE / AGENT / f\"{PROVIDER}{suffix}.jsonl\"\n\nif not trials_file.exists():\n    raise FileNotFoundError(\n        f\"Trials file not found: {trials_file}\\n\"\n        f\"Run: bun run trials -- --agent {AGENT} --search-provider {PROVIDER}\"\n    )\n\nprint(f\"\\nLoading: {trials_file}\")\nprint(\"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "## Load Trials Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load and Parse Trials Data\n",
    "with open(trials_file) as f:\n",
    "    trials = [json.loads(line) for line in f]\n",
    "\n",
    "df = pd.DataFrame(trials)\n",
    "\n",
    "print(f\"‚úì Loaded {len(df)} prompts with trial data\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nSample record:\")\n",
    "print(json.dumps(trials[0], indent=2))\n",
    "\n",
    "# Extract key metrics\n",
    "df['k'] = df['trials'].apply(len)\n",
    "df['prompt_short'] = df['id'].str[:40] + '...'\n",
    "\n",
    "print(f\"\\nüìà DATASET SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Prompts: {len(df)}\")\n",
    "print(f\"Trials per prompt (k): {df['k'].iloc[0]}\")\n",
    "print(f\"Total evaluations: {df['k'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pass-rates'></a>\n",
    "## Pass Rate Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Pass Rate Distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Histogram of pass rates\n",
    "ax1.hist(df['passRate'], bins=20, color='#3498db', alpha=0.7, edgecolor='black')\n",
    "ax1.axvline(df['passRate'].mean(), color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "ax1.axvline(df['passRate'].median(), color='green', linestyle='--', linewidth=2, label='Median')\n",
    "ax1.set_xlabel('Pass Rate', fontsize=12)\n",
    "ax1.set_ylabel('Number of Prompts', fontsize=12)\n",
    "ax1.set_title('Distribution of Pass Rates Across Prompts', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Pass@k vs Pass^k scatter\n",
    "ax2.scatter(df['passAtK'], df['passExpK'], s=100, alpha=0.6, color='#3498db')\n",
    "ax2.plot([0, 1], [0, 1], 'r--', alpha=0.3, label='Perfect Reliability')\n",
    "ax2.set_xlabel('pass@k (Capability)', fontsize=12)\n",
    "ax2.set_ylabel('pass^k (Reliability)', fontsize=12)\n",
    "ax2.set_title('Capability vs Reliability', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä PASS RATE STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Mean Pass Rate: {df['passRate'].mean():.1%}\")\n",
    "print(f\"Median Pass Rate: {df['passRate'].median():.1%}\")\n",
    "print(f\"Std Dev: {df['passRate'].std():.1%}\")\n",
    "print(f\"\\nAlways Pass (100%): {(df['passRate'] == 1.0).sum()} prompts\")\n",
    "print(f\"Sometimes Pass (0-100%): {((df['passRate'] > 0) & (df['passRate'] < 1.0)).sum()} prompts\")\n",
    "print(f\"Never Pass (0%): {(df['passRate'] == 0.0).sum()} prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='capability-reliability'></a>\n",
    "## Capability vs Reliability Frontier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Capability vs Reliability Analysis\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Color by pass rate\n",
    "scatter = ax.scatter(df['passAtK'], df['passExpK'], \n",
    "                     c=df['passRate'], cmap='RdYlGn', \n",
    "                     s=150, alpha=0.7, edgecolors='black', linewidth=1)\n",
    "\n",
    "# Add diagonal line (perfect reliability)\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.3, linewidth=2, label='Perfect Reliability')\n",
    "\n",
    "# Add quadrant lines\n",
    "ax.axhline(0.5, color='gray', linestyle=':', alpha=0.3)\n",
    "ax.axvline(0.5, color='gray', linestyle=':', alpha=0.3)\n",
    "\n",
    "# Annotate quadrants\n",
    "ax.text(0.75, 0.75, 'High Capability\\nHigh Reliability', \n",
    "        ha='center', va='center', fontsize=10, alpha=0.5, \n",
    "        bbox=dict(boxstyle='round', facecolor='green', alpha=0.1))\n",
    "ax.text(0.25, 0.25, 'Low Capability\\nLow Reliability', \n",
    "        ha='center', va='center', fontsize=10, alpha=0.5,\n",
    "        bbox=dict(boxstyle='round', facecolor='red', alpha=0.1))\n",
    "ax.text(0.75, 0.25, 'High Capability\\nLow Reliability (Flaky)', \n",
    "        ha='center', va='center', fontsize=10, alpha=0.5,\n",
    "        bbox=dict(boxstyle='round', facecolor='orange', alpha=0.1))\n",
    "\n",
    "ax.set_xlabel('pass@k (Probability of success in k attempts)', fontsize=12)\n",
    "ax.set_ylabel('pass^k (Probability of k consecutive successes)', fontsize=12)\n",
    "ax.set_title(f'Capability vs Reliability Frontier\\n{AGENT.upper()} - {PROVIDER.upper()} ({len(df)} prompts)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_xlim(-0.05, 1.05)\n",
    "ax.set_ylim(-0.05, 1.05)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(scatter, ax=ax)\n",
    "cbar.set_label('Pass Rate', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° INTERPRETATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"pass@k = 1 - (1 - p)^k  (Can solve with k attempts)\")\n",
    "print(\"pass^k = p^k            (Solves k times in a row)\")\n",
    "print(\"\\nIdeal: Top-right (high both) = Capable AND reliable\")\n",
    "print(\"Concern: Top-left or bottom-right = Flaky (inconsistent)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='flakiness'></a>\n",
    "## Flakiness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Identify Flaky Prompts\n",
    "# Flakiness score: high passAtK but low passExpK = inconsistent\n",
    "df['flakiness'] = df['passAtK'] - df['passExpK']\n",
    "\n",
    "# Show top 10 flakiest prompts\n",
    "flaky = df.nlargest(10, 'flakiness')[['id', 'passRate', 'passAtK', 'passExpK', 'flakiness']]\n",
    "\n",
    "print(\"üî• TOP 10 FLAKIEST PROMPTS\")\n",
    "print(\"=\"*70)\n",
    "print(flaky.to_string(index=False))\n",
    "\n",
    "# Plot flakiness distribution\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.hist(df['flakiness'], bins=20, color='#e74c3c', alpha=0.7, edgecolor='black')\n",
    "ax.axvline(df['flakiness'].mean(), color='blue', linestyle='--', linewidth=2, label='Mean')\n",
    "ax.set_xlabel('Flakiness Score (pass@k - pass^k)', fontsize=12)\n",
    "ax.set_ylabel('Number of Prompts', fontsize=12)\n",
    "ax.set_title('Flakiness Distribution', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° High flakiness = Agent can solve but not consistently\")\n",
    "print(\"   Consider: Retries, prompt refinement, or different agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='difficulty'></a>\n",
    "## Prompt Difficulty Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Hardest and Easiest Prompts\n",
    "easiest = df.nlargest(10, 'passRate')[['id', 'passRate', 'passAtK', 'passExpK']]\n",
    "hardest = df.nsmallest(10, 'passRate')[['id', 'passRate', 'passAtK', 'passExpK']]\n",
    "\n",
    "print(\"‚úÖ TOP 10 EASIEST PROMPTS (Highest Pass Rate)\")\n",
    "print(\"=\"*70)\n",
    "print(easiest.to_string(index=False))\n",
    "\n",
    "print(\"\\n‚ùå TOP 10 HARDEST PROMPTS (Lowest Pass Rate)\")\n",
    "print(\"=\"*70)\n",
    "print(hardest.to_string(index=False))\n",
    "\n",
    "# Plot pass rate ranking\n",
    "df_sorted = df.sort_values('passRate', ascending=False).reset_index(drop=True)\n",
    "df_sorted['rank'] = range(1, len(df_sorted) + 1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.plot(df_sorted['rank'], df_sorted['passRate'], marker='o', linestyle='-', alpha=0.6)\n",
    "ax.axhline(0.5, color='red', linestyle='--', alpha=0.3, label='50% Threshold')\n",
    "ax.fill_between(df_sorted['rank'], 0, df_sorted['passRate'], alpha=0.2)\n",
    "ax.set_xlabel('Prompt (Sorted by Pass Rate)', fontsize=12)\n",
    "ax.set_ylabel('Pass Rate', fontsize=12)\n",
    "ax.set_title('Pass Rate by Prompt Difficulty', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='heatmap'></a>\n",
    "## Per-Prompt Trial Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Trial Results Heatmap\n",
    "# Build matrix of trial results (rows = prompts, columns = trials)\n",
    "trial_matrix = []\n",
    "prompt_ids = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    # Extract pass/fail for each trial (1 = pass, 0 = fail)\n",
    "    trial_results = [1 if t.get('score', 0) >= 0.65 else 0 for t in row['trials']]\n",
    "    trial_matrix.append(trial_results)\n",
    "    prompt_ids.append(row['prompt_short'])\n",
    "\n",
    "trial_matrix = np.array(trial_matrix)\n",
    "\n",
    "# Plot heatmap\n",
    "fig, ax = plt.subplots(figsize=(14, max(10, len(df) * 0.3)))\n",
    "sns.heatmap(trial_matrix, cmap=['#e74c3c', '#2ecc71'], cbar=False,\n",
    "            yticklabels=prompt_ids, xticklabels=range(1, trial_matrix.shape[1] + 1),\n",
    "            ax=ax, linewidths=0.5, linecolor='white')\n",
    "\n",
    "ax.set_xlabel('Trial Number', fontsize=12)\n",
    "ax.set_ylabel('Prompt', fontsize=10)\n",
    "ax.set_title(f'Trial Results Heatmap\\n{AGENT.upper()} - {PROVIDER.upper()} (Green = Pass, Red = Fail)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Each row = one prompt, each column = one trial\")\n",
    "print(\"   Consistent green rows = reliable prompts\")\n",
    "print(\"   Mixed rows = flaky prompts\")\n",
    "print(\"   Consistent red rows = unsolvable prompts (for this agent)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Recommendations\n",
    "\n",
    "### Key Metrics\n",
    "- **pass@k**: Probability of at least one success in k trials (capability)\n",
    "- **pass^k**: Probability of k consecutive successes (reliability)\n",
    "- **Flakiness**: pass@k - pass^k (inconsistency measure)\n",
    "\n",
    "### Use Cases\n",
    "1. **Production Deployment**: Choose agents with high pass^k (reliability)\n",
    "2. **Prompt Engineering**: Focus on flaky prompts (can work but inconsistent)\n",
    "3. **Agent Selection**: Compare reliability across agents for specific task types\n",
    "4. **Regression Testing**: Track if reliability drops over time\n",
    "\n",
    "### Related Analysis\n",
    "- **Comparison Analysis**: See `comparison.ipynb` for quality rankings\n",
    "- **Custom Analysis**: Load raw JSONL files for trajectory inspection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}